# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
# what we will emulate:
esm_name <- "MIROC6"
esm_experiment <- 'ssp245'
# How many times do we want to draw full generated ensembles:
Ndraws <- 1
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
ead.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%nrow()
# what we will emulate:
esm_name <- "NorCPM1"
esm_experiment <- 'ssp245'
# How many times do we want to draw full generated ensembles:
Ndraws <- 1
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) %>%
select(-acceptable) ->
archive_subset1
# what does this archive look like:
archive_subset1 %>%
select(experiment, ensemble) %>%
distinct() %>%
group_by(experiment) %>%
summarize(n_complete_ensemble_members = n()) %>%
ungroup %>%
kable()
# Archive data including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) %>%
select(-acceptable) ->
archive_subset2
# what does this archive look like:
archive_subset2 %>%
select(experiment, ensemble) %>%
distinct() %>%
group_by(experiment) %>%
summarize(n_complete_ensemble_members = n()) %>%
ungroup %>%
kable()
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
nrow(acceptable_archive_entries_from_path_metadata)
# what we will emulate:
esm_name <- "UKESM1-0-LL"
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
nrow(acceptable_archive_entries_from_path_metadata)
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) -> x
nrow(x)
x %>% na.omit %>% nrow()
x %>% na.omit
x
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
nrow(x)
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
#filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
nrow(x)
x %>% na.omit()
nrow(x %>% na.omit)
# what we will emulate:
esm_name <- "CanESM5"
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) ->
x
x%>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
x
x[116,]
x[116,]$pr_zstore
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) ->
x
x[116,]
x%>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
nrow(x)
nrow(acceptable_archive_entries_from_path_metadata)
# what we will emulate:
esm_name <- "MPI-ESM1-2-LR"
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
nrow(acceptable_archive_entries_from_path_metadata)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
i
esm_name
length(esm_name_vec)
j
i
# a wrapper script for iteratively compiling `Notebook8-emulate-validate.Rmd` for each ESM separately.
# what we will emulate:
esm_name_vec <- c("ACCESS-ESM1-5",
"CanESM5",
#"GISS-E2-1-G", # no psl or pr data on pangeo
"MIROC6",
"MPI-ESM1-2-HR",
"MPI-ESM1-2-LR",
# "NorCPM1", # don't have archive data past 2025/raw tgav data past 2029
"UKESM1-0-LL")
esm_experiment_vec <- c('ssp245', 'ssp370')
# How many times do we want to draw full generated ensembles:
Ndraws <- 200
name_ind <- 1
exp_ind <- 1
esm_name <- esm_name_vec[name_ind]
esm_experiment <- esm_experiment_vec[exp_ind]
print(name_ind)
print(exp_ind)
print(esm_name)
print(esm_experiment)
source('~/.active-rstudio-document', echo=TRUE)
i <- 1
generated_ensemble_draw <- read.csv(files[i], stringsAsFactors = FALSE)
# Values of the generated data and then the metrics
generated_ensemble_draw %>%
group_by(tol, archive, draw) %>%
summarize(mean_gen = mean(value),
var_gen = sd(value)) %>%
ungroup %>%
mutate(time_period = 'hist+future',
mean_target = mean_target,
var_target = var_target,
bias = abs(mean_target - mean_gen),
E1 = bias/var_target,
E2 = var_gen/var_target) ->
metrics
metrics
generated_ensemble_draw
files[i]
head(generated_ensemble_draw)
plotting_tb
generated_ensemble_draw %>%
group_by(tol, archive, draw) %>%
summarize(mean_gen = mean(value),
var_gen = sd(value)) %>%
ungroup %>%
mutate(time_period = 'hist+future',
mean_target = mean_target,
var_target = var_target,
bias = abs(mean_target - mean_gen),
E1 = bias/var_target,
E2 = var_gen/var_target) ->
metrics
generated_ensemble_draw %>%
filter(year <= 2014) %>%
group_by(tol, archive, draw) %>%
summarize(mean_gen = mean(value),
var_gen = sd(value)) %>%
ungroup %>%
mutate(time_period = 'hist',
mean_target = mean_target_hist,
var_target = var_target_hist,
bias = abs(mean_target - mean_gen),
E1 = bias/var_target,
E2 = var_gen/var_target) ->
metrics_hist
generated_ensemble_draw%>%
filter(year > 2014) %>%
group_by(tol, archive, draw) %>%
summarize(mean_gen = mean(value),
var_gen = sd(value)) %>%
ungroup %>%
mutate(time_period = 'future',
mean_target = mean_target_future,
var_target = var_target_future,
bias = abs(mean_target - mean_gen),
E1 = bias/var_target,
E2 = var_gen/var_target) ->
metrics_future
generated_ensemble_draw %>%
select(stitching_id, tol, archive, draw) %>%
distinct %>%
group_by(tol, archive, draw) %>%
summarise(n_stitched = n()) %>%
ungroup ->
tmp
bind_rows(tmp %>%
left_join(metrics, by = c('tol', 'archive', 'draw')),
tmp %>%
left_join(metrics_hist, by = c('tol', 'archive', 'draw')) ,
tmp %>%
left_join(metrics_future, by = c('tol', 'archive', 'draw')))  ->
plotting_tbl
head(plotting_tbl)
unique(plotting_tbl$draw)
source('~/.active-rstudio-document', echo=TRUE)
generated_metrics <- list()
files <- list.files(path = paste0('generated_ensembles/', esm_name),
pattern = paste0('generated_ensembles_', esm_experiment),
full.names = TRUE)
generated_tgavs <- list()
for (i in 1:length(files)){
generated_tgavs[[i]] <- read.csv(files[i], stringsAsFactors = FALSE)
}
generated_tgavs <- do.call(rbind, generated_tgavs)
head(generated_tgavs)
# make a helper function for calculating all the jumps because you have Nyears-1 jumps and
# dply functions don't like that.
get_jumps <- function(data){
data.frame(left_year = data$year[1:(length(data$year)-1)],
jump = diff(data$value),
abs_jump = abs(diff(data$value))) %>%
mutate(variable = unique(data$variable),
stitching_id = unique(data$stitching_id),
tol = unique(data$tol),
archive = unique(data$archive),
draw = unique(data$draw),
jump_year = if_else(left_year %in% window_end_years, 'window_transition', 'in_window')) ->
x
return(x)
}
# calculate the yearly jumps
generated_list <- split(generated_tgavs,
list(generated_tgavs$tol,
generated_tgavs$archive,
generated_tgavs$draw,
generated_tgavs$stitching_id),
drop=TRUE)
lapply(generated_list, get_jumps) %>%
do.call(rbind, .) ->
generated_jumps
window_start_years <- unique(archive_subset1$start_yr)
window_end_years <- unique(archive_subset1$end_yr)
generated_list <- split(generated_tgavs,
list(generated_tgavs$tol,
generated_tgavs$archive,
generated_tgavs$draw,
generated_tgavs$stitching_id),
drop=TRUE)
lapply(generated_list, get_jumps) %>%
do.call(rbind, .) ->
generated_jumps
target_list <- split(original_data,
list(original_data$model,
original_data$experiment,
original_data$ensemble),
drop = TRUE)
lapply(target_list, get_jumps) %>%
do.call(rbind, .) ->
target_jumps
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
