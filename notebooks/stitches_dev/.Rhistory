archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble) ->
acceptable_archive_entries_from_path_metadata
acceptable_archive_entries_from_path_metadata
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows)
archive_data
head(archive_data)
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model))
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
filter(across(c(model, experiment, ensemble)) %in% acceptable_archive_entries_from_path_metadata)->
archive_subset1
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
filter(c(model, experiment, ensemble) %in% acceptable_archive_entries_from_path_metadata)->
archive_subset1
c(model, experiment, ensemble) %in% acceptable_archive_entries_from_path_metadata)
c(model, experiment, ensemble) %in% acceptable_archive_entries_from_path_metadata
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
acceptable_archive_entries_from_path_metadata
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", rmeove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) ->
archive_subset1
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) ->
archive_subset1
archive_subset1
# what we will emulate:
esm_name <- 'CanESM5'
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
# what we will emulate:
esm_name <- "MIROC6"
esm_experiment <- 'ssp245'
# How many times do we want to draw full generated ensembles:
Ndraws <- 1
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
ead.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%nrow()
# what we will emulate:
esm_name <- "NorCPM1"
esm_experiment <- 'ssp245'
# How many times do we want to draw full generated ensembles:
Ndraws <- 1
# Archive data not including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Don't match to any ensemble members in the target experiment:
dplyr::filter(experiment != unique(target_data$experiment) ) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) %>%
select(-acceptable) ->
archive_subset1
# what does this archive look like:
archive_subset1 %>%
select(experiment, ensemble) %>%
distinct() %>%
group_by(experiment) %>%
summarize(n_complete_ensemble_members = n()) %>%
ungroup %>%
kable()
# Archive data including the target experiment:
archive_data %>%
# Only match to the same ESM:
dplyr::filter(model == unique(target_data$model)) %>%
# Exclude ssp534-over:
dplyr::filter(experiment != 'ssp534-over') %>%
# eliminate any individual realizationXexperiment runs that don't have all the years:
group_by(experiment, variable,   model  , ensemble) %>%
mutate(nWindows = n()) %>%
ungroup %>%
filter(nWindows == 28) %>%
select(-nWindows) %>%
# Final filter to make sure we only use the experiment*ensemble members with pangeo
# path metadata for netcdfs for all varaibles of interest
unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) %>%
select(-acceptable) ->
archive_subset2
# what does this archive look like:
archive_subset2 %>%
select(experiment, ensemble) %>%
distinct() %>%
group_by(experiment) %>%
summarize(n_complete_ensemble_members = n()) %>%
ungroup %>%
kable()
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
nrow(acceptable_archive_entries_from_path_metadata)
# what we will emulate:
esm_name <- "UKESM1-0-LL"
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>% nrow()
nrow(acceptable_archive_entries_from_path_metadata)
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) -> x
nrow(x)
x %>% na.omit %>% nrow()
x %>% na.omit
x
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
nrow(x)
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
#filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) -> x
x
nrow(x)
x %>% na.omit()
nrow(x %>% na.omit)
# what we will emulate:
esm_name <- "CanESM5"
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) ->
x
x%>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
x
x[116,]
x[116,]$pr_zstore
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) ->
x
x[116,]
x%>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
nrow(x)
nrow(acceptable_archive_entries_from_path_metadata)
# what we will emulate:
esm_name <- "MPI-ESM1-2-LR"
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
# Time series of the raw global mean temperature anomaly, this is the data that is
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv",
stringsAsFactors = FALSE) %>% dplyr::select(-X)
# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several
# steps were taken in python to transform it into the "chunked" data and save it
# so that we do not have to repeate this process so many times.
archive_data <- read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE)
# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
filter(experiment == esm_experiment) ->
target_data
# The corresponding unsmoothed original data for validation
tgav_data %>%
filter(model == unique(target_data$model) & experiment == unique(target_data$experiment))  %>%
select(-file, -timestep, -grid_type) ->
original_data
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv",
stringsAsFactors = FALSE,
na.strings = c("", " ", "NA")) %>%
rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
filter(model == unique(target_data$model) )  %>%
select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
# cut out any rows that don't have paths to netcdfs for all variables:
na.omit %>%
select(model, experiment, ensemble)  %>%
unite("acceptable", c(model, experiment, ensemble), sep="~")->
acceptable_archive_entries_from_path_metadata
nrow(acceptable_archive_entries_from_path_metadata)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
source('~/Documents/task11a-topdown-clim-ML/stitches/notebooks/stitches_dev/knit_notebook8_byESM.R', echo=TRUE)
i
esm_name
length(esm_name_vec)
j
i
