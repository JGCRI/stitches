---
title: "add new behavior for matching when no matches occur"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
    toc_float: yes
    number_sections: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r, echo=FALSE}
# required packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)
```
# Goal

update matching to handle cases with 0 matches in a window satisfying the tolerance.

Solution: return the nearest match regardless of the tolerance, with a print statement.




# Set up

```{r}


# what we will emulate:
esm_name <- "CanESM5"
esm_experiment <- 'ssp245'

# How many times do we want to draw full generated ensembles:
Ndraws <- 1

print(esm_name)
print(esm_experiment)
```
 
```{r, message=FALSE, warning=FALSE}

# Then load the functions we will want to use, these are currently written in R and will be translated into python. 
source("functions/nearest_neighbor_matching.R") # the function match_neighborhood is defined here
source("functions/permuting_matches.R")  # the function permute_stitching_recipe is defined here
source("functions/stitching_functions.R")  # the function stitch_global_mean is definend here 
source("functions/gridded_recipe.R") # wrappers for the work flow from matching to output csv needed by python for 
                                     # layering in gridded data to output as netcdf

set.seed(1)
```


Load the inputs that will be used. 
In addition to SSP534-over having enough data issues to fully exclude, _at least some_ of the realizations for SSP434 and SSP460 just don't have future data at all for CanESM5. And for other models, some realizations have odd windows (Like a 2012-2014 window) and those realizations are excluded as well. We need to go back to pangeo and fix this for sure, but for now, I'll work with the archive I have. Because we don't want to lose too much of the archive now, I will only remove those realizations and not a full experiment for a single bad data point.

```{r}
# Time series of the raw global mean temperature anomaly, this is the data that is 
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv", 
                      stringsAsFactors = FALSE) %>% dplyr::select(-X)


# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several 
# steps were taken in python to transform it into the "chunked" data and save it 
# so that we do not have to repeate this process so many times. 
read.csv(paste0('inputs/', esm_name, '_archive_data.csv'), stringsAsFactors = FALSE) %>%
  # Exclude ssp534-over:
  dplyr::filter(experiment != 'ssp534-over') %>%
  # eliminate any individual realizationXexperiment runs that don't have all the years:
  group_by(experiment, variable,   model  , ensemble) %>%
  mutate(nWindows = n()) %>%
  ungroup %>%
  filter(nWindows == 28) %>%
  select(-nWindows) ->
  archive_data


# The target data - all ssp245 realizations (smoothed because it's the target for
# matching)
archive_data %>%
  filter(experiment == esm_experiment) ->
  target_data


# The corresponding unsmoothed original data for validation
tgav_data %>%  
  filter(model == unique(target_data$model) & experiment == unique(target_data$experiment)) %>%
  filter(ensemble %in% unique(target_data$ensemble))  %>%
  select(-file, -timestep, -grid_type) -> 
  original_data
```

We also need to prepare the file of path metadata for pangeo. This is a csv file that contains the pangeo paths to different variables of interest. By selecting the paths for `tas`, `psl`, and `pr`, keeping only those experimentXensemble combinations that have defined paths to netcdfs for all three variables, and then using the resulting experiment X ensemble combinations as an additional filter on the archives we use for matching, we make sure that we can then stitch together gridded data for all 3 of these variables no matter what our generated ensembles are. Adding or removing variables of interest is relatively trivial. For now, the path metadata file has information for `tas`, `psl`, `pr`, `tasmin`, and `tasmax`. 

```{r}
# The main pangeo file of path metadata so that we can filter the archives to include only
# the experiment*ensemble members with netcdfs for all variables of interest
read.csv("inputs/pangeo_path_metadata_tas_psl_pr_tmax_tmin.csv", 
         stringsAsFactors = FALSE,
         na.strings = c("", " ", "NA")) %>%  
  rename(model = source_id, experiment = experiment_id, ensemble = member_id) %>%
  filter(model == unique(target_data$model) )  %>%
  select(model, experiment, ensemble, table_id, tas_zstore, psl_zstore, pr_zstore) %>%
  # cut out any rows that don't have paths to netcdfs for all variables:
  na.omit %>%
  select(model, experiment, ensemble)  %>%
  unite("acceptable", c(model, experiment, ensemble), sep="~")->
  acceptable_archive_entries_from_path_metadata

```


# Set up archives

The archive without the target experiment included is obviously more likely to have target points that don't get matches, so we save time and just work with that

```{r}
# Archive data not including the target experiment:
archive_data %>% 
  # Only match to the same ESM:
  dplyr::filter(model == unique(target_data$model)) %>%  
  # Don't match to any ensemble members in the target experiment:
  dplyr::filter(experiment != unique(target_data$experiment) ) %>%
  # Final filter to make sure we only use the experiment*ensemble members with pangeo
  # path metadata for netcdfs for all varaibles of interest
  unite("acceptable", c(model, experiment, ensemble), sep="~", remove=FALSE) %>%
  filter(acceptable %in% unique(acceptable_archive_entries_from_path_metadata$acceptable)) %>%
  select(-acceptable) ->
  archive_subset1 

# what does this archive look like:
archive_subset1 %>% 
  select(experiment, ensemble) %>%
  distinct() %>% 
  group_by(experiment) %>% 
  summarize(n_complete_ensemble_members = n()) %>%
  ungroup %>%
  kable()

```


# Get the sd of the smoothed target data

This is roughly our max tolerance - we assume that beyond this, the generated ensemble members are no good. Re-smooth the raw target data for each ensemble member (9 year window).

```{r}

rollAvg <-  function(x,n){
  y <- stats::filter(x,rep(1/n,n), sides=2)
  ind <- which(is.na(y))

  for(i in 1:length(ind)){
    left <- max(1,ind[i] - (n-1) /2)
    right <- min( max(ind), ind[i] + (n-1) /2)
    y[ind[i]] <- mean(x[left:right])
  }

  y
}

# smooth the Tgav for each ensemble member
list_orig_data <- split(original_data, 
                        original_data$ensemble)

lapply(list_orig_data, function(ens_mem){
  return(  ens_mem %>%
    mutate(smooth_tgav = as.numeric( rollAvg(value,9))))
  }) %>%
  do.call(rbind, .) -> 
  smoothed

smoothed %>%
  mutate(sd_across_year_and_ens = sd(smooth_tgav)) %>%
  group_by(ensemble) %>%
  mutate(sd_across_year_each_ens = sd(smooth_tgav)) %>%
  ungroup %>%
  group_by(year) %>%
  mutate(sd_across_ens_each_year =sd(smooth_tgav) ) %>%
  ungroup %>%
  mutate(mean_of_sd_across_ens_each_year = mean(sd_across_ens_each_year),
         max_of_sd_across_ens_each_year = max(sd_across_ens_each_year),
         min_of_sd_across_ens_each_year = min(sd_across_ens_each_year)) %>%
  select(ensemble, model, variable, experiment, year,
         sd_across_ens_each_year,
         mean_of_sd_across_ens_each_year,
         max_of_sd_across_ens_each_year,
         min_of_sd_across_ens_each_year,
         sd_across_year_each_ens,
         sd_across_year_and_ens) %>%
  distinct->
  tol_cutoff_info

kable(head(tol_cutoff_info))
kable(tol_cutoff_info %>%
        select(model, variable, experiment,
               min_of_sd_across_ens_each_year,
               mean_of_sd_across_ens_each_year,
               max_of_sd_across_ens_each_year) %>%
        distinct)

rm(smoothed)
rm(list_orig_data)
```
We will include the mean and max value as vertical lines on our plots since the cutoff for tolerances that create 'good' Tgav ensembles is probably in between these values. 


# do the matching for a range of tolerances

```{r}
start <- Sys.time()


tolerances <-  c(0.1)

for (tol in tolerances){

  # The matching to the archive without ssp245 results
  match_neighborhood(target_data = target_data,
                     archive_data = archive_subset1,
                     tol=tol) ->
    match

  match_neighborhood(target_data = target_data,
                     archive_data = archive_subset1,
                     tol=0) ->
    nearest_neighbors
  
}

end <- Sys.time()

print(end-start)
```
* remember matches aren't slow, but doing the draws is

# print out info

First, make sure every target ensemble member has a full set of matches (n_target_windows after matching is 28):
```{r}
match %>% 
  select(target_variable, target_experiment, target_ensemble, target_model, target_year) %>%
  distinct %>%
  group_by(target_variable, target_experiment, target_ensemble, target_model) %>%
  summarise(n_target_windows = n()) %>%
  ungroup ->
  x 

  print(paste('All target_years in all ensemble members have a full set of matches:',
             all(x$n_target_windows==28)))
  
  rm(x)
```

Now take a look at which points got matches outside of the tolerance

```{r}

match %>% 
  filter(dist_l2 > tolerances[1]) %>%
  select(-target_start_yr, -target_end_yr, -archive_start_yr, -archive_end_yr,
         -target_variable, -archive_variable, -archive_model) %>%
  mutate(tol = tolerances[1]) %>%
  select(target_model, tol, target_experiment, target_ensemble, 
         target_year, target_fx, target_dx,
         archive_experiment, archive_ensemble,
         archive_year, archive_fx, archive_dx, dist_fx, dist_dx, dist_l2) ->
  matches_outside_tol 

kable(matches_outside_tol)
kable(quantile(matches_outside_tol$dist_l2, probs =c(0, 0.25, 0.5,
                                                     0.55, 0.6, 0.65, 0.7,
                                                     0.75, 0.8, 0.85, 0.9, 0.95, 0.98, 0.99, 1)))
kable(quantile(nearest_neighbors$dist_l2, probs =c(0, 0.25, 0.5, 0.75, 0.8, 0.85, 0.9, 0.95, 0.98, 0.99, 1)))
```


# conclusions about matching for this model-experiment

- For this particular model and experiment, the target_years that fail to get matches within tolerance are generally later 21st century (2050 and on). 

- Based on looking at the `fx` and `window_size*dx` distances returned for the nearest neighbor, for many of these points, it's the dx dimension that it's hard to find a close enough match for. Some points it is fx and some points it is both dimensions roughly equally. For an ssp126 that has a tiny bit of overshoot, that probably makes sense.

- Still, for a range of `tol` values, we ran into this issue even with ssp245 and ssp370, so it is good we have this fix.


As a point of interest, how do the `dist_l2` values of these points compare to the SD of the smoothed target data?

```{r}
# The sd info again:
kable(tol_cutoff_info %>%
        select(model, variable, experiment,
               min_of_sd_across_ens_each_year,
               mean_of_sd_across_ens_each_year,
               max_of_sd_across_ens_each_year) %>%
        distinct)

# the range of dx distances dist_dx = windowsize*abs(target_dx-archive_dx)
kable(quantile(matches_outside_tol$dist_dx))

#  the range of fx distances dist_fx=abs(target_df-archive_fx)
kable(quantile(matches_outside_tol$dist_fx))

# the range of l2 distances dist_l2 = sqrt( dist_fx^2 + dist_dx^2):
kable(quantile(matches_outside_tol$dist_l2))


```



