---
title: "Handling duplicate matches across multiple realizations"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
    toc_float: yes
    number_sections: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

# Goal

Avoid envelope collapse in generated realizations, whether they correspond to a single target trajectory or multiple target trajectories. 

- We've developed a function to re-match points within a single generated ensemble member that has the same archive point matched for multiple target years: `remove_duplicates`. Within a single generated realization for a single target trajectory, this looks for any target-years that got the same archive match. If there are any, a subsetted archive for re-matching is created, as the original archive minus all of the matched points (so that new duplicates to other target years don't get introduced on rematching). The year with the smallest distance to a duplicate match keeps it, and the other target years that had that duplicate match get re-matched to their nearest-neighbor on the subsetted archive; iterate until no two target-years in a single trajectory have the same archive match. (Archive Match means the experimentXrealizationXyear - the individual point in the archive.) 

- Prior to this notebook, we do the neighborhood matching, we do the permutation sampling, and then we call `remove_duplicates` and then just stitch together from those resulting recipes.

- the permutation function that gives us our recipes does enforce that each sampled recipe is unique, but by doing things in this order, the results of `remove_duplicates` can actually introduce some non-uniqueness in our set of sampled recipes.

- On this branch, we have moved the call to `remove_duplicates` into the permutation sampling function to fix that.

- Then the main goal is updating the permutation sampling function so that, for a single target trajectory, multiple generated realizations won't all get the same archive match to the same target year (e.g. don't want every 2070 to be the same across generated ensemble members).

- Note that if a target period only has one match in the entire archive (even on a neighborhood), then that's what the constructed ensemble members get. There will be collapse. When working with the full archive and even a modest neighborhood size (0.1degC), this doesn't really happen but just worth noting.


- Then we have to figure out what to do for looking not just across generated realizations for a single target, but across generated realizations across multiple targets. 


# Notes

## single target

So basically what we are saying is that, when generating multiple ensemble members (by calling `permute_stitching_recipes`), once a (target-year, archive-match) pair has been used in a stitching recipe,it can't be used again. That is, two generated ensembles can't have (2075, realization 1 2070). So it's just a matter of removing that point from the `matched_points` set iteratively as each new ensemble member is constructed. 

But this does then take us from having a huge number of all possible mixes and matches in terms of generated realizations to only being able to generate a much smaller number of ensemble members: the target year with the smallest number of matching archive points dictates the number of realizations that you can generate according to throwing out each (target-year, archive-match) pair once they've been used. So going from like 10^42 possible mixes and matches to just 10 because target year 1989 only had 10 archive points matched in the neighborhood. Every time you do that sampling, those 10 might be different. But you can't have a generated ensemble of more than 10 realizations if each realization has to have a different 1989 value to avoid envelope collapse.

We _can_ define a re-matching condition on a subsetted archive (with existing matches removed) the same way `remove_duplicates` does if we want more than those 10; the issue there is that by definition, those new matches are less good and you are more likely to create unacceptable generated realizations. So for now, not doing it. 

## across targets

Say we want to generate multiple new ensemble members for each of a target trajectory of realization 1 and a target trajectory of realization 4. Because those targets are pretty similar, we risk envelope collapse again. THinking of just one generated ensemble member for each target, we don't want the case of having (target realization 1 year 2070) getting the same archive match as (target realization 4 year 2070), in addition to the fact that we don't want that to happen to multiple generated realizations for any of the targets. It does appear to have the same solution then - remove each (target-year, archive-match) pair as they get used, regardless of which trajectory the target-year came from.

So, we construct generated realizations iteratively. We start with the target that has the most limited number of collapse-free generated ensemble members. Construct those, removing each constructed (target-year, archive-match) as we go. Then move on to the next target with the next smallest number of collapse-free generated ensemble members. Repeat until we're either done or out of collapse-free ensembles to construct. I could see a scenario where we run out of collapse-free ensembles before we run out of targets to match, just depending on archive/sampling/etc/.

Note that because we want to enforce this across targets, it's really not parallellizable except across _experiments_. Also in general, with the way the code has been written, it's probably best to call it separately for each experiment of interest being worked with. Just because this whole duplication elimination stuff is not optional and it wouldn't make sense that points used in the construction of a new ssp245 realization couldn't be used in the construction of a new ssp585. 

# Set Up 
```{r, message=FALSE, warning=FALSE}
# required packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)


# The load the functions we will want to use, these are currently written in R and will be translated into python. 
source("functions/nearest_neighbor_matching.R") # the function match_neighborhood is defined here
source("functions/stitching_functions.R")  # the function stitch_global_mean is definend here 
source("functions/permuting_matches.R")
source("functions/gridded_recipe.R")
```


Load the inputs. 

```{r}
# Time series of the raw global mean temperature anomaly, this is the data that is 
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv", 
                      stringsAsFactors = FALSE) %>% dplyr::select(-X)


# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several 
# steps were taken in python to transform it into the "chunked" data and save it 
# so that we do not have to repeate this process so many times. 
archive_data <- read.csv('inputs/archive_data.csv', stringsAsFactors = FALSE)


# This is chunked smooth tgav anomaly for a single model/experiment/ensemble
# member, saved for our convenience. If you decide that you want to work with 
# a different different target data subset you can subset it from the
# archive_data data frame. This file is SSP245 Realization 1.
target_data <- read.csv("inputs/target_data.csv", stringsAsFactors = FALSE)

# For plotting
tgav_data %>%  
  filter(model == unique(target_data$model) & experiment == unique(target_data$experiment) & 
           ensemble == unique(target_data$ensemble)) -> 
  original_data1


# target data 2: ssp245 realization 4
archive_data %>%
  filter(experiment == 'ssp245' & ensemble == 'r4i1p1f1') ->
  target_data2
 
# for plotting
tgav_data %>%  
  filter(model == unique(target_data2$model) & experiment == unique(target_data2$experiment) & 
           ensemble == unique(target_data2$ensemble)) -> 
  original_data2

```


# Define the archive for testing

The full archive minus the two trajectories we are emulating:

```{r}
archive_data %>% 
  dplyr::filter(model == unique(target_data$model)) %>%  
  dplyr::filter(experiment != 'ssp534-over' |
                (experiment == 'ssp245' & ensemble == 'r4i1p1f1') |
                (experiment == 'ssp245' & ensemble == 'r1i1p1f1')) ->
  archive_subset
```


# Do matching to that archive and sample recipes

With the `remove_duplicates` call now in `permute_stitching_recipes` to enforce that no matches are repeated within a single generated ensemble member and that the recipes put out by `permute_stitching_recipes` are in fact unique.

```{r, warning=FALSE, message=FALSE}
matched_data <- match_neighborhood(target_data = bind_rows(target_data, target_data2),
                   archive_data = archive_subset,
                   tol=0.1)

set.seed(400)
matched_data%>%
  # Convert them to a sample of individual Tgav Recipes:
  permute_stitching_recipes(N_matches = 5, matched_data = .,
                            archive = archive_subset) ->
  recipes
```



The following wrapper re-does all of the matching and saves a data frame with stripped down information that can be ingested by python code to layer in the gridded netcdfs. This wrapper does not output enough information for the R function `stitch_global_means`.

```{r}
set.seed(400)
gridded_recipes_for_python <- generate_gridded_recipe(target_data = bind_rows(target_data, target_data2), 
                                    archive_data = archive_subset, n = 4, tol = 0.1)
write.csv(gridded_recipes_for_python, file = "./gridded_recipes_for_python.csv", row.names = FALSE)
```


## Stitch and plot

```{r}

lapply(split(recipes, recipes$stitching_id),
       function(df){
         df %>% 
           stitch_global_mean(match = ., data = tgav_data) %>%
           mutate(stitching_id = unique(df$stitching_id))
       }) %>%
  do.call(rbind, .) ->
  new_tgavs


# Put in one data frame with orig data to plot
new_tgavs %>%
  mutate(tosep = stitching_id) %>%
  separate(tosep, c('experiment', 'ensemble', 'trash'), sep = "~") %>%
  select(-trash) %>%
  left_join(select(bind_rows(original_data1, original_data2), -timestep, -grid_type, -file) %>%
              rename(orig_val = value), 
            by = c('year', 'variable', 'experiment', 'ensemble')) ->
  plotting_tgavs

# Plot
ggplot() + 
  facet_wrap(~interaction(plotting_tgavs$experiment, plotting_tgavs$ensemble), nrow = 2) +
  geom_line(data = plotting_tgavs , 
                          aes(year, value, color = as.factor(stitching_id))) +
  scale_color_brewer(palette = 'Spectral') +
  geom_line(data = plotting_tgavs, aes(x = year, y = orig_val), color = "grey", size = 0.4) + 
  theme_bw() + 
  labs(title = "Comparison of ESM and Stitched Data",
       x = "Year", y = "Deg C")
```



# Make sure that what we're seeing around 2000 isn't collapse

```{r}
plotting_tgavs %>%
  filter(year >= 1985, year <=2015) -> 
  plotting_tgavs

ggplot() + 
  facet_wrap(~interaction(plotting_tgavs$experiment, plotting_tgavs$ensemble), nrow = 2) +
  geom_line(data = plotting_tgavs , 
                          aes(year, value, color = as.factor(stitching_id))) +
  scale_color_brewer(palette = 'Spectral') +
  geom_line(data = plotting_tgavs, aes(x = year, y = orig_val), color = "grey", size = 0.4) + 
  theme_bw() + 
  labs(title = "Comparison of ESM and Stitched Data",
       x = "Year", y = "Deg C")
```



# Construct many more realizations

When matching a target realization of ssp245 realization 4 to a neighborhood of 0.1deg C, one of the target years only had 4 matches. So, it is only possible to sample 4 trajectories without having collapse in that target window so we only construct 4.

With our iterative collapse-free construction (and a neighborhood size of 0.1degc for matching),  these are a maximum sized sample for our two target trajectories. A different seed would select different trajectories but these are the largest number of trajectories we can get without any collapse:

```{r, echo=FALSE, warning=FALSE}

set.seed(925)

start <- Sys.time()
# Convert them to a sample of individual Tgav Recipes:
matched_data %>%
  permute_stitching_recipes(N_matches = 100, matched_data = .,
                            archive = archive_subset) ->
  recipes
end <- Sys.time()


lapply(split(recipes, recipes$stitching_id),
       function(df){
         df %>% 
           stitch_global_mean(match = ., data = tgav_data) %>%
           mutate(stitching_id = unique(df$stitching_id))
       }) %>%
  do.call(rbind, .) ->
  new_tgavs


# Put in one data frame with orig data to plot
new_tgavs %>%
  mutate(tosep = stitching_id) %>%
  separate(tosep, c('experiment', 'ensemble', 'trash'), sep = "~") %>%
  select(-trash) %>%
  left_join(select(bind_rows(original_data1, original_data2), -timestep, -grid_type, -file) %>%
              rename(orig_val = value), 
            by = c('year', 'variable', 'experiment', 'ensemble')) ->
  plotting_tgavs

# Plot
ggplot() + 
  facet_wrap(~interaction(plotting_tgavs$experiment, plotting_tgavs$ensemble), nrow = 2) +
  geom_line(data = plotting_tgavs , 
                          aes(year, value, color = stitching_id)) +
  geom_line(data = plotting_tgavs, aes(x = year, y = orig_val), color = "grey", size = 0.5) + 
  theme_bw() + 
  labs(title = "Comparison of ESM and Stitched Data",
       x = "Year", y = "Deg C")


ggplot() + 
  facet_wrap(~interaction(plotting_tgavs$experiment, plotting_tgavs$ensemble), nrow = 2) +
  geom_line(data = plotting_tgavs , 
                          aes(year, value, color = stitching_id)) +
  geom_line(data = plotting_tgavs, aes(x = year, y = orig_val), color = "grey", size = 0.5) + 
  theme_bw() + 
  coord_cartesian(xlim=c(1985, 2015), ylim = c(-1, 1.25)) + 
  labs(title = "Comparison of ESM and Stitched Data",
       x = "Year", y = "Deg C")
```