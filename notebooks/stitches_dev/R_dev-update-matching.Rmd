---
title: "Updating Matching - acs"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
    toc_float: yes
    number_sections: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

# Goal
We want to update so that matching returns not just nearest neighbor matches, but any matches within a specified tolerance `tol` of that match. We take this approach because it allows us to keep a biophysical interpretation: `tol` has units of degrees C, as does our euclidean distance metric `dist_l2` between target and archive `(fx, windowsize*dx)` points. This is one benefit to doing the matching on `windowsize*dx` instead of just `dx`: sensible units that we can use for other things.

The alternative would be to return the K nearest neighbors for some specified K. This is considered undesirable because:

1. K may be arbitrary, whereas we can set `tol` for example to be the interannual variability of the specific ESM.
2. There's no way to control if that K-th match is actually close at all to the nearest neighbor match. Maybe the 5th nearest neighbor is a full degree away from the nearest neighbor match, who knows?

Note that this does mean we _only_ go in the direction of the nearest neighbor match. That means if there's a 2nd nearest match in a completely different direction and kind of far from the nearest neighbor, it _won't_ get captured in this. That's a potential area for next development (say, find 3 nearest neighbors and then do the `tol` neighborhood about each) but for right now, sticking with just a neighborhood of the nearest neighbor lets us work on developing code for the permutation of all matches and stitching of those permutations into many realizations.

Extending to do neighborhoods of K nearest neighbors from there should be straightforward.
    
# Set Up 
```{r, message=FALSE, warning=FALSE}
# required packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)

# New required library to do the combinations
library(RcppAlgos)

# The load the functions we will want to use, these are currently written in R and will be translated into python. 
source("nearest_neighbor_matching.R") # the function match_nearest_neighbor is defined here
source("stitching_functions.R")  # the function stitch_global_mean is definend here 
```


Load the inputs that will be used in the tutorial. 

```{r}
# Time series of the raw global mean temperature anomaly, this is the data that is 
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv", 
                      stringsAsFactors = FALSE) %>% dplyr::select(-X)


# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several 
# steps were taken in python to transform it into the "chunked" data and save it 
# so that we do not have to repeate this process so many times. 
archive_data <- read.csv('inputs/archive_data.csv', stringsAsFactors = FALSE)


# This is chunked smooth tgav anomaly for a single model/experiment/ensemble
# member, saved for our convenience. If you decide that you want to work with 
# a different different target data subset you can subset it from the
# archive_data data frame. This file is SSP245 Realization 1.
target_data <- read.csv("inputs/target_data.csv", stringsAsFactors = FALSE)

```

# Matching 


Use the function `match_neighborhood` to match the target and archive data to one another. This function takes each `(fx, windowsize*dx)` point in the target data and returns the nearest-neighbor archive point as well as every archive point in a specified neighborhood of radius `tol` of that nearest neighbor. 

Right now the user will have to manually subset the contents of the archive so that we are not matching to other ESMs or the target scenario.


```{r}
archive_data %>% 
  dplyr::filter(model == unique(target_data$model)) %>%  
  # dplyr::filter(experiment != unique(target_data$experiment) &
  #                 experiment!= 'ssp534-over') ->
  dplyr::filter(experiment %in% c("ssp126", "ssp585")) -> 
  archive_subset
```

Now that we have the subset of the archive we want to work with, we can use it in the nearest neighbor. 

Match the target and archive data! We use the default value of `tol=0.01` degC for these tests, as the l2 distance for the vast majority of nearest-neighbor matches is O(0.01).

Matches will return a rather large data frame, with 21 columns. It will contain information about the target and archive data matched together. Columns that contain information about the distance of the matched pairs are `dist_dx`, `dist_fx`, and `dist_l2`.  `dist_dx` and `dist_fx` reflect the distance between the `window-size*dx` and `fx` components. `dist_l2` contains the euclidean distance, this is the distance that we use to select the nearest neighbor. 

## Tests

Test that the neighborhood function returns the same results as the nearest neighbor function for a neighborhood of radius 0:
```{r}
matched_data1 <- match_nearest_neighbor(target_data = target_data, archive_data = archive_subset)

matched <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0)

print(paste("New matching with tol=0 agrees with prev nearest neighbor matching:", all(matched_data1$dist_l2 == matched$dist_l2)))

```

Make sure that the l2 distances returned for each target point are actually close to each other with max distance between any two of the archive points = `2*tol`. (The max distance of `2*tol` would be two points on the edge of the .01degC neighborhood about the nearest neighbor, directly across from each other).

```{r}
matched <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.01)

# For each target point, all matched archive points must be close enough:

# split out each target point
matched_list <- split(matched,
                      f=list(matched$target_variable, matched$target_experiment,
                             matched$target_ensemble, matched$target_model,
                             matched$target_start_yr, matched$target_end_yr,
                             matched$target_year, matched$target_fx, 
                             matched$target_dx),
                      drop = TRUE)

# for each target point, compute the pairwise difference among all `dist_l2` values 
# and the minimimum l2 distance (which is the nearest neighbor match):
lapply(matched_list, function(df){
  pairwise <- as.vector(dist(df$dist_l2))
  if (length(pairwise) == 0){
    max_pairwise <- 0
  } else {
    max_pairwise <- max(pairwise)
  }
 
  min_l2 <- min(df$dist_l2)
  nmatches <- length(unique(df$archive_year))
  
  
  return(df %>% 
    dplyr::group_by(target_variable, target_experiment,
                    target_ensemble, target_model,
                    target_start_yr, target_end_yr,
                    target_year, target_fx, target_dx) %>%
    dplyr::summarise(max_pairwise = max_pairwise,
                     min_l2 = min_l2,
                     n_matches = nmatches) %>%
    ungroup)
}) %>% do.call(rbind, .) ->
  x

print(paste("all matched points are withing 2*tol of each other:", max(x$max_pairwise) <= 0.02))

groups <- names(matched)[grepl('target', names(matched))]
x %>%
  left_join(select(matched_data1, target_variable, target_experiment,
                    target_ensemble, target_model,
                    target_start_yr, target_end_yr,
                    target_year, target_fx, target_dx, dist_l2),
            by = groups) -> 
  y

print(paste("the nearest neighbor is included in matched points:", all(y$min_l2 == y$dist_l2)))
```

```{r, echo=FALSE}
# clean up from tests
rm(matched)
rm(matched_data1)
rm(matched_list)
rm(groups)
rm(x)
rm(y)
```

# Turning all permutations of matches into stitching recipes

## Get our Matches and take a look
Do the basic matching of interest with a neighborhood of 0.01 degC (to keep the number of matches low but more than just nearest neighbor):

```{r}
matched_data <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.01)
head(matched_data)
```


```{r}
matched_data %>%
  ggplot() + 
  geom_density(aes(dist_dx, fill = "dist_dx"), alpha = 0.5) + 
  geom_density(aes(dist_fx, fill = "dist_fx"), alpha = 0.5) + 
  geom_density(aes(dist_l2, fill = "dist_l2"), alpha = 0.5) + 
  theme_bw() + 
  labs(title = "Compare Distance", 
       y = "Density", x = "Distance Value")
```



### Visualize matches In the space where matching occurs with equal axes

To visually ensure that we are getting the matches we think we should.

```{r}
ggplot(data = matched_data) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, 9*dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, 9*archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, 9*target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = 9*target_dx, xend = archive_fx, yend =  9*archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  xlim(-2, 7) + ylim(-2, 7)+
  theme_bw() + 
  labs(y = "windowsize*dx (degC)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive" )
```

### visualize matches in the raw data
As a zoom in to see more detail.
```{r}
ggplot(data = matched_data) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = target_dx, xend = archive_fx, yend =  archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  theme_bw() + 
  labs(y = "dx (rate of change per chunk, degC/year)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive" )
```


### Visualize matching in a larger neighborhood just for fun
With `tol=0.05` deg C, you can see that we begin to get significantly more matches, especially in the historical period where the archive points are so clustered together anyway.

```{r, echo=FALSE}
matched_data05 <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.05)

ggplot(data = matched_data05) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = target_dx, xend = archive_fx, yend =  archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  theme_bw() + 
  labs(y = "dx (rate of change per chunk, degC/year)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive, neighborhood = 0.05degC" )
```



## Do the permutations of matches

We want all permutations of matches that can cover the full 1850-2100 of the target data. Work on matches with `tol=0.01` to have fewer to work with for function development and testing.

This is a genuinely computationally challenging ask and I think we'll want to rely heavily on packages. That will make the translation from R to Python a little trickier, as we'll have to find a different package with comparable behavior but I think it's the only chance we have of doing it efficiently for any great number of matches. 

```{r}

```







# Questions and next steps:

- do we really want to be applying the tolerance to history the way we are, or do we only want to match history directly? Ie update the matching and or permutations code to only operate on the future years.

Next steps:
1. evaluation of these tgav's
2. in parallel translate to python
3. add in the layer of matching the gridded netcdf data in to construct new monthly T and P netcdfs.
4. Test those.
5. Publish???
6. Look at matching on more complex things than smoothed Tgav (ML?) and look at layering in and evaluating (ML?) daily data?
7. Look at adding variables beyond T and P,from ESMs.
8. Loook at organizing results from impacts models, crop and hydrology models, so that they could be layered in as well. Don't think it would work for stuff like Forest productivity because may be too much memory, unless that's working strictly with derivatives and not actual values. Probably less memory in the rate of change than in the actual time series of values.