---
title: "Updating Matching - acs"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
    toc_float: yes
    number_sections: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

# Goal
We want to update so that matching returns not just nearest neighbor matches, but any matches within a specified tolerance `tol` of that match. We take this approach because it allows us to keep a biophysical interpretation: `tol` has units of degrees C, as does our euclidean distance metric `dist_l2` between target and archive `(fx, windowsize*dx)` points. This is one benefit to doing the matching on `windowsize*dx` instead of just `dx`: sensible units that we can use for other things.

The alternative would be to return the K nearest neighbors for some specified K. This is considered undesirable because:

1. K may be arbitrary, whereas we can set `tol` for example to be the interannual variability of the specific ESM.
2. There's no way to control if that K-th match is actually close at all to the nearest neighbor match. Maybe the 5th nearest neighbor is a full degree away from the nearest neighbor match, who knows?

Note that this does mean we _only_ go in the direction of the nearest neighbor match. That means if there's a 2nd nearest match in a completely different direction and kind of far from the nearest neighbor, it _won't_ get captured in this. That's a potential area for next development (say, find 3 nearest neighbors and then do the `tol` neighborhood about each) but for right now, sticking with just a neighborhood of the nearest neighbor lets us work on developing code for the permutation of all matches and stitching of those permutations into many realizations.
Extending to do neighborhoods of K nearest neighbors from there should be straightforward.

Additionally note that in terms of setting the tolerance, it may be more of a function of how many realizations you want than anything like ESM interannual variability. A `tol=0.01` degC returns a huge number possible stitching recipes for CanESM from an archive containing only SSP126 and SSP585. The vast majority of those are from the historical period. If you want the historical period in our stitched Tgav to be identical to the historical period in our target Tgav, it's far fewer stitching recipes. 


# Set Up 
```{r, message=FALSE, warning=FALSE}
# required packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)

# New required library to do the combinations
# library(RcppAlgos)

# The load the functions we will want to use, these are currently written in R and will be translated into python. 
source("nearest_neighbor_matching.R") # the function match_nearest_neighbor is defined here
source("permuting_matches.R") # the functions for taking matched data and creating stitching recipes are here
source("stitching_functions.R")  # the function stitch_global_mean is definend here 

```


Load the inputs that will be used in the tutorial. 

```{r}
# Time series of the raw global mean temperature anomaly, this is the data that is 
# going to be stitched together. Historical data is pasted into every scenario. No
# smoothing has been done.
tgav_data <- read.csv("inputs/main_raw_pasted_tgav_anomaly_all_pangeo_list_models.csv", 
                      stringsAsFactors = FALSE) %>% dplyr::select(-X)


# A chunked smoothed tgav anomaly archive of data. This was tgav_data but several 
# steps were taken in python to transform it into the "chunked" data and save it 
# so that we do not have to repeate this process so many times. 
archive_data <- read.csv('inputs/archive_data.csv', stringsAsFactors = FALSE)


# This is chunked smooth tgav anomaly for a single model/experiment/ensemble
# member, saved for our convenience. If you decide that you want to work with 
# a different different target data subset you can subset it from the
# archive_data data frame. This file is SSP245 Realization 1.
target_data <- read.csv("inputs/target_data.csv", stringsAsFactors = FALSE)

```

# Matching 


Use the function `match_neighborhood` to match the target and archive data to one another. This function takes each `(fx, windowsize*dx)` point in the target data and returns the nearest-neighbor archive point as well as every archive point in a specified neighborhood of radius `tol` of that nearest neighbor. 

Right now the user will have to manually subset the contents of the archive so that we are not matching to other ESMs or the target scenario.


```{r}
archive_data %>% 
  dplyr::filter(model == unique(target_data$model)) %>%  
  # dplyr::filter(experiment != unique(target_data$experiment) &
  #                 experiment!= 'ssp534-over') ->
  dplyr::filter(experiment %in% c("ssp126", "ssp585")) -> 
  archive_subset
```

Now that we have the subset of the archive we want to work with, we can use it in the nearest neighbor. 

Match the target and archive data! We use the default value of `tol=0.01` degC for these tests, as the l2 distance for the vast majority of nearest-neighbor matches is O(0.01).

Matches will return a rather large data frame, with 21 columns. It will contain information about the target and archive data matched together. Columns that contain information about the distance of the matched pairs are `dist_dx`, `dist_fx`, and `dist_l2`.  `dist_dx` and `dist_fx` reflect the distance between the `window-size*dx` and `fx` components. `dist_l2` contains the euclidean distance, this is the distance that we use to select the nearest neighbor. 

## Tests

Test that the neighborhood function returns the same results as the nearest neighbor function for a neighborhood of radius 0, meaning we can get rid of `match_nearest_neighbor`:
```{r}
matched1 <- match_nearest_neighbor(target_data = target_data, archive_data = archive_subset)

matched <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0,
                              drop_hist_duplicates = FALSE)

print(paste("New matching with tol=0 agrees with prev nearest neighbor matching:", all(matched1$dist_l2 == matched$dist_l2)))

```

Make sure that the l2 distances returned for each target point are actually close to each other with max distance between any two of the archive points = `2*tol`. (The max distance of `2*tol` would be two points on the edge of the .01degC neighborhood about the nearest neighbor, directly across from each other).

```{r}
matched <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.01,
                              drop_hist_duplicates = FALSE)

# For each target point, all matched archive points must be close enough:

# split out each target point
matched_list <- split(matched,
                      f=list(matched$target_variable, matched$target_experiment,
                             matched$target_ensemble, matched$target_model,
                             matched$target_start_yr, matched$target_end_yr,
                             matched$target_year, matched$target_fx, 
                             matched$target_dx),
                      drop = TRUE)

# for each target point, compute the pairwise difference among all `dist_l2` values 
# and the minimimum l2 distance (which is the nearest neighbor match):
lapply(matched_list, function(df){
  pairwise <- as.vector(dist(df$dist_l2))
  if (length(pairwise) == 0){
    max_pairwise <- 0
  } else {
    max_pairwise <- max(pairwise)
  }
 
  min_l2 <- min(df$dist_l2)
  
  
  return(df %>% 
    dplyr::group_by(target_variable, target_experiment,
                    target_ensemble, target_model,
                    target_start_yr, target_end_yr,
                    target_year, target_fx, target_dx) %>%
    dplyr::summarise(max_pairwise = max_pairwise,
                     min_l2 = min_l2) %>%
    ungroup)
}) %>% do.call(rbind, .) ->
  x

print(paste("all matched points are withing 2*tol of each other:", max(x$max_pairwise) <= 0.02))

groups <- names(matched)[grepl('target', names(matched))]
x %>%
  left_join(select(matched1, target_variable, target_experiment,
                    target_ensemble, target_model,
                    target_start_yr, target_end_yr,
                    target_year, target_fx, target_dx, dist_l2),
            by = groups) -> 
  y

print(paste("the nearest neighbor is included in matched points:", all(y$min_l2 == y$dist_l2)))
```

```{r, echo=FALSE}
# clean up from tests
rm(matched)
rm(matched1)
rm(matched_list)
rm(groups)
rm(x)
rm(y)
```


# Matching - historical period considerations

Matching to neighborhoods of the nearest-neighbor match for each point of target data comes with a few tricky details in the historical period.


First, there's a question of whether all of the neighborhood matches in the historical period are 'True' matches.


Second, there's the question of whether we want variations in our stitched data from the actual historical data of the target data.


## Historical false duplicates
Do the basic matching of interest with a neighborhood of 0.01 degC (to keep the number of matches low but more than just nearest neighbor):

```{r}
matched_data <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.01,
                              drop_hist_duplicates = FALSE)
head(matched_data)
```

Note that there are two kinds of matches in the historical period:

1. true near matches, eg target 1850 getting matches from  different realizations and or years.
2. false duplicates: target 1850 gets 1872 data from realization 13 of SSP126 and SSP585. The metadata of these archive values are different, but the actual data values are identical because we just pasted in the same historical data to every Experiment. 

So we write a helper function to drop false duplicates like these in the historical, while keeping any of the true near matches.


```{r}
matched_data <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.01,
                              drop_hist_duplicates = TRUE)
```

## History identical to the target data
The above steps allow for variations in the historical period, away from the nearest-neighbor matches in the historical period. Which, for target data from the actual CMIP experiments, means identical history when nearest neighbor matches are in place. 

If we want to only work with stitched realizations that are identical to the historical period of the target data, we would do:

```{r}
# TODO: Make this a helper function or an input argument to match_neighborhood
# instead of coding like this. 
# Also unsure if 2015 is the cutoff year we would want since the data is smoothed. 
# So 2015 is a 'future' year but the smoothed values still include some history. 

matched_hist <-  match_neighborhood(target_data = filter(target_data, year < 2015),
                                    archive_data = archive_subset,
                                    tol = 0)
matched_fut <-  match_neighborhood(target_data = filter(target_data, year >= 2015),
                                    archive_data = archive_subset,
                                    tol = 0.01)
bind_rows(matched_hist, matched_fut) %>%
  arrange(target_year) ->
  matched_data_identical_hist

# Note there's no issue of false historical duplicates here because we only kept the nearest neighbor
# to begin with.
kable(matched_data_identical_hist)
```

# Visualize Matches
## In the space where matching occurs with equal axes
These are the matches that allow departures from the target data's historical values.
To visually ensure that we are getting the matches we think we should.

```{r}
ggplot(data = matched_data) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, 9*dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, 9*archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, 9*target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = 9*target_dx, xend = archive_fx, yend =  9*archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  xlim(-2, 7) + ylim(-2, 7)+
  theme_bw() + 
  labs(y = "windowsize*dx (degC)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive" )
```

## visualize matches in the raw data
These are the matches that allow departures from the target data's historical values.
As a zoom in to see more detail.
```{r}
ggplot(data = matched_data) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = target_dx, xend = archive_fx, yend =  archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  theme_bw() + 
  labs(y = "dx (rate of change per chunk, degC/year)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive" )
```


### Visualize matching in a larger neighborhood just for fun
With `tol=0.05` deg C, you can see that we begin to get significantly more matches, especially in the historical period where the archive points are so clustered together anyway.

```{r, echo=FALSE}
matched_data05 <- match_neighborhood(target_data = target_data, archive_data = archive_subset, tol = 0.05)

ggplot(data = matched_data05) + 
  # Add the subset of the archive data that was read into the matching process. 
  geom_point(data = archive_subset, aes(fx, dx, color = "no match"), alpha = 0.4) + 
  # Add the matched together points and make clear which points are matched with which. 
  geom_point(aes(archive_fx, archive_dx, color = "matched archive data")) + 
  geom_point(aes(target_fx, target_dx,  color = "target data"), alpha = 0.4) + 
  # Add lines between the matched values
  geom_segment(aes(x = target_fx, y = target_dx, xend = archive_fx, yend =  archive_dx), alpha = 0.4) +
 scale_color_manual(values = c("matched archive data" = "red", 
                               "target data" = "blue", "no match" = "grey"))+
  theme_bw() + 
  labs(y = "dx (rate of change per chunk, degC/year)", 
       x = "fx (value of median temperature per chunk, degC)", 
       title = "Matching between target and archive, neighborhood = 0.05degC" )
```



# Do the permutations of matches
These are the matches that allow departures from the target data's historical values.

We want all permutations of matches that can cover the full 1850-2100 of the target data. Work on matches with `tol=0.01` to have fewer to work with for function development and testing.

This is a genuinely computationally challenging ask and I think we'll want to rely heavily on packages. That will make the translation from R to Python a little trickier, as we'll have to find a different package with comparable behavior but I think it's the only chance we have of doing it efficiently for any great number of matches. 

Let's take a look at how many matches we have for each target year in our `tol=0.01` example:
```{r}

num_matches <- get_num_perms(matched_data)

print(paste("There are a total of:", num_matches[[1]], "stitching recipes"))

kable(num_matches)
```
So already so many permutations even with most target years still only having their nearest neighbor match. 

There is a good chance that a lot of those matches in the historical period are identical data, since we pasted in the same historical values for the different experiments. So it is probably more efficient to force historical to be identical to the target data for now.

We _can_ decrease that some by only using the nearest neighbor match in the historical period.
```{r}

num_matches_identical_hist <- get_num_perms(matched_data_identical_hist)

print(paste("There are a total of:", num_matches_identical_hist[[1]], "stitching recipes"))

kable(num_matches_identical_hist)
```
Obviously far, far fewer when you require the historical period to match the target data identically and only allow new matches in the future period.

So We will be working with `matched_data_identical_hist` (identical history) while figuring out how to write the permutations functions.


# Turning all permutations of matches stitching recipes into stitched Tgavs

The table with all permutations of stitching recipes will probably have some kind of Id column like `stitch_id`. 
So for each `stitch_id`, the stitching process should be exactly like before, so we probably will only have to update
the stitching to do `split-lapply-do.call(rbind)` on the `stitch_id` to make all of the new Tgavs.

# Questions and next steps:

- do we really want to be applying the tolerance to history the way we are, or do we only want to match history directly? Code above for both ways


Next steps:
1. evaluation of these tgav's
2. in parallel translate to python
3. add in the layer of matching the gridded netcdf data in to construct new monthly T and P netcdfs.
4. Test those.
5. Publish???
6. Look at matching on more complex things than smoothed Tgav (ML?) and look at layering in and evaluating (ML?) daily data?
7. Look at adding variables beyond T and P,from ESMs.
8. Loook at organizing results from impacts models, crop and hydrology models, so that they could be layered in as well. Don't think it would work for stuff like Forest productivity because may be too much memory, unless that's working strictly with derivatives and not actual values. Probably less memory in the rate of change than in the actual time series of values.